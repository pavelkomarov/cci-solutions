One
I'd keep the end-of-day stock info in a distributed, scalable data store, like S3. I'd make this a web service queryable through a REST API call that can take params and returns JSON. That call would either hit a "serverless" front end like AWS Lambda to spin up a process to go fetch and return the info from the data store, or it would hit a load balancer, behind which would be several servers capable of running the job. For monitoring there are AWS services like CloudWatch to check on the health of the network and set up alerts if something goes wrong.

Two
Each Profile is an object. It will have a collection of other profiles, which are its connections. It can have other information like "Likes" or "Skills" or "Posts". These other pieces of information could be large, so they should be their own objects, collections of which a profile contains references to, so they can fetched as needed. Information that's old or doesn't need to be fetched often can be in cold (or colder) storage. Facebook has that neat "collect all my data for download" feature, to be in compliance with EU law, and it takes a day or so for them to give you the .zip to download, because presumably they're using AWS Glacier or something similar underneath. Really, handling the "very large" aspect is just a matter of not trying to keep track of or display everything at once. If a profile's collection of posts grows too large, split it across a couple machines. You're almost definitely going to want to keep data in duplicate or triplicate somewhere so it can't be lost, unless there's tolerability for loss in some cases. For instance, who cares about a few old "likes" except maybe our data team who are trying to extract insights from them? For finding shortest path between friends use BFS, but the branching factor is like several hundred here, so use a two-sided BFS to keep the explosion of nodes down. Searching nodes is extra expensive, because we'll probably have to go fetch them from other machines in our network, because distributed database. Optimizations: batch queries for friend profiles by which machine those are on. Keep people from similar geographic locations on the same machine so you're less likely to have to query other machines.

Three
We're probably going to have multiple instances of the web crawler going, started by some kind of manager. It's expensive to keep and query against a global set of what's been explored before, but it's also expensive to explore things over again. Each crawler instance should keep its own explored set up to some reasonable size and then probably terminate and dump this to the global store, along with unexplored leaf nodes that might serve as the beginnings for new crawler instances. We can probably exploit cluster effects in the network of websites to make an estimate of how often we need to query that global set vs just the local one. If a page's content or location has been updated since the last crawl, we'd ideally like to update our entry for it rather than make a new one. We might have distributed hash(url) -> hash(content) and hash(content) -> hash(url) maps to figure this out. The solutions make clear that determining which pages have been visited before is really fuzzy, so a similarity-metric-based system, where pages more similar to ones that have been crawled before get lower priority vs truly different-seeming pages, can make sense.

Four
You could put them in a very large database and see what collides. This trades space for time. Or you could iterate through looking for duplicates, which trades time for space. You could preprocess and split urls across several machines by prefix. Each node in the network can keep many nodes of the trie and refer to other nodes in the network at leaves when things get too big. Rebalancing that would be a nightmare, though, in the event our urls end up getting replaced over time. You could preprocess by sorting, and then partition across machines in a very flat way. Because duplicates end up next to each other, it's then quick to see them, but again rebalancing could be annoying. If it's just urls, and we have 10 billion, then that's on the order of 10 GB if we have a byte per url. Urls are probably more like 100 bytes each, so you're looking at a Terabyte, which today could fit on a single machine if you just vertically scale. If you have a big machine and just want to do this job the once, I'd just make a big set. The hash of a url might be much smaller than 100 bytes anyway. A clever option given in the solutions is to store sub-files containing all urls with hash % some number == x. This way you split the problem and can then read sub-file by sub-file in to memory, never needing very much at once.

Five
There could be multiple levels of caching going on. Each of our 100 servers can keep a small cache of maybe the most recent or most popular queries so they can respond immediately if they happen to get a hit. There could be a database connected to our 100 machines that's a little more thorough. If querying against this database can be made significantly cheaper than a processSearch call, then it's a win. Clever way from solutions: distribute this cache database across the 100 machines by hash(query) % 100, so the 100 machines query each other for cache entries. This is tricky if the "answers" processSearch returns are changing. Depends on the time scale of that change. You might want to toss cache entries after some clock runs out, or you might only keep the most recent n of them. Clever use of data structures to implement a cache, from the solutions: Keep cache entries in a linked list, where old items roll off the tail as new items are added to the head. If an item already in the list is queried, that item gets cut out and put at the head. A map keeps track of hash(cache item)->linked list node. As things roll off, you can then easily del from the map, and if you need to find something in the linked list, you don't have to iterate from the head. You also don't need to explicitly keep track of information's freshness in the most-recent-n sense, because it's implicitly encoded in its linked list position. You probably do still want each node to have an age, from when it was first created in the cache, and bypass the cache to a true processSearch call if it's old enough to possibly be stale.

Six
You could keep a monster list of all products in the universe and then when I want to see "Sports Equipment" or some finer category, iterate through generating the sub-list. Probably better to store the results of such work in product entries themselves, or store product entries in several tables by category. As new information pertaining to an item's rank comes in, we can then go update the tables accordingly. Good point that no one's life hangs in the balance if we're a bit delayed in reflecting a product's ranking, so probably agglomerate new sales info together for a while, then run a single job to apply updates to reflect all of it. I made the assumption we're keeping track of all-time position, so the analytics tables keep the absolute number of things sold, and when new sales are made, you basically increment that. The solutions emphasize the actual way you'd do some of this, like keeping purchase numbers for things over the last 7 days in circular arrays, having tables of products by category, or writing purchases as logs to folders for each category and then running a mapreduce routine to integrate all that information.

Seven
First you're going to need data feeds. You'll have to have an interface for connecting to banks, credit cards, investment accounts, etc. and pulling in transactions/dividends/price changes, etc. I'd try to unify this internally, so that we can connect a new bank by creating a plugin to handle the specifics of their system, but once the data is in our ecosystem, our programs and programmers can assume it's handled in a fairly uniform way, according to known schemas. Ideally we'd want transactions and new information to flow in to the system seamlessly, without manual input from users. But information doesn't have to be perfectly up to date, so we can run jobs to get new transactions nightly or something. For spending habits, it's useful to take advantage of category or other metadata our partners might collect. The schema should allow for this. Mint has a feature where you can change a purchase's category, and I've done this a LOT to try to get more accurate analytics on my spending. It would be great if, based on a user's inputs this way, we can learn to better classify what things are. Just supervised learning, really. I think Mint tries to do this, but it doesn't work great, possibly because each user isn't feeding that much supervision back in. We might do better by choosing a model with pretty high bias, so it trains faster (though possibly less accurately), or by using aggregate transaction info from many users, though there are *so* many different kinds of transaction when you do this compared to single users, who have established purchase habits, that who a purchase is made by might be an important feature too. Prettty charts and graphs are a must. They're the reason I use Mint. Bar, pie, line, whatever. Let the data guys go ham, but keep the absolute number of types of plot to maybe at most a dozen so Mint doesn't become the Bloomberg Terminal. Based on whether a user's habits suddenly change, or they're saving more, or spending more in some category, we can hazard a guess about which financial products or behavioral tweaks they might like and make suggestions. Data for each person lives in a data store and gets accessed by jobs as necessary for display. Old info will be less relevant and might be in colder storage. The solutions specify a task queue to do things like pull bank data and see whether users are exceeding budgets. I was thinking just do these things at set times, but a priority queue could allow you to not do work that may not matter, like updating someone's account who never logs in. The solutions also stress emails. I hate emails from these sorts of apps, so it didn't even cross my mind.

Eight
We'll need a big database (or collection of files). People's pastes will live there, and we'll need a map from url->content (or reference to it) we're supposed to surface. Pastebin has a great feature where you can specify when your pastes expire, so let's definitely have that to save space. This means the url->content will be constantly changing. Having a hierarchical url->machine that manages that range of urls->reference to content will help us scale. We probably want more than one manager machine for redundancy so one going down doesn't kill the whole system. I might key machines by the date a paste is meant to expire so that when one fills up we can just start a new one, and when all of one's urls expire, it becomes free again. If we were to do it alphabetically, then we'd be constantly shifting info from one backing database machine to another. Actually, I think some rebalancing is unavoidable either way. Frequently-accessed pages might be cached before you ever get to the database machines, or might be cached by them in memory instead of on disk, so they can respond more quickly. This cache can just be rolling through time, because if a page is popular, the dynamics of virality often mean it's only popular for a while. For generating urls, I'd probably do it randomly. Long enough and the risk of collisions is low. But if you do get a collision, increment the ascii until you find an empty slot. People might also want to select their own urls. This is fine, because then the entry in our tables just becomes that instead of whatever we would have randomly set it to.
